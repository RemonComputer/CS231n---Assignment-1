\documentclass[]{article}

%opening
\title{Support Vector Machine Vectorized proof}
\author{Remon Kamal}

\usepackage{amsmath} 
\usepackage[utf8]{inputenc}
\usepackage{mathtools}

\begin{document}

\maketitle

\begin{abstract}

This is the proof of the vectorized form of SVM, I do this with
lots of Matrix Math, I hope you like my latex skills

\end{abstract}

\section{proof}

\begin{align}
	loss &= \frac{1}{N} \sum_{i=0}^{N-1} loss_i + \lambda \left\lVert W \right\rVert_2 \label{eq:1}
\end{align}
Where N is number of examples, W is the weight matrix and $\lambda$ is the regression weight 
\begin{align}
	loss_i &= \sum_{k=0}^{C-1} T_{ik} \label{eq:2}
\end{align}
Where  $loss_i$ is the loss per example, $T_{ik}$ is loss of class k in example i
\begin{align}
	T_{ik} &= \begin{cases}
	0 & \text{, $margin_{ik} \le 0$} \\
	S_{ik} - S_{i y_j} + 1 & \text{, $margin_{ik} > 0$}
	\end{cases} \label{eq:3}
\end{align}
Where $margin_{ik}$ is the margin loss of class k in example i, and it is given by
\begin{align}
	margin_{ik} &= S_{ik} - S_{i y_j} + 1 
\end{align}
And  $S_{ik}$ is the score of class k in example i
\begin{align}
	S_{ik} &= X_{i} W_{k} \label{eq:5}
\end{align}
\begin{align}
	W_{k}  & \rightarrow \text{k-Coloum vector of W matrix} \nonumber
\end{align}
\begin{align}
	W_{k} &= W u_{k} \label{eq:6}
\end{align}
\begin{align}
	u_{k} & \rightarrow \text{Unit coloum vector in direction of k} \nonumber
\end{align}
Substituting From \eqref{eq:6} in \eqref{eq:5} we get:
\begin{align}
	S_{ik} &= X_{i} W u_{k} \label{eq:7}
\end{align}
Taking the $\nabla_W$ of \eqref{eq:1} we have
\begin{align}
	\nabla_W loss &= \frac{1}{N} \nabla_W loss_i + \lambda \nabla_W ||W||_{2}^{2} \label{eq:8}
\end{align}
We divide the left part of \eqref{eq:8} and process each part separately
\begin{align}
	\nabla_W ||W||_{2}^{2} &= 2 W \label{eq:9}
\end{align}
\begin{align}
	\nabla_W loss_i &= \sum_{k=0}^{c-1} \nabla_W T_{ik}
\end{align}
where c is the number of classes in our problem
\begin{align}
	\nabla_W T_{ik}	&= \begin{cases}
	0 & \text{, $margin_{ik} \le 0$} \\
	\nabla_W S_{ik} - \nabla_W S_{i y_j} & \text{, $margin_{ik} > 0$}
	\end{cases}
\end{align}
\begin{align}
	{\left( \nabla_W \right)}_{lm} &= \frac{ \partial }{\partial W_{lm}} \label{eq:12}
\end{align}
From equation \eqref{eq:7} and substituting in equation \eqref{eq:12} we get:  
\begin{align}
	{\left( \nabla_W S_{ik} \right)}_{lm} &= \frac{ \partial }{\partial W_{lm}} \left( X_i W u_k\right) \nonumber  \\
	&= X_i \frac{\partial}{\partial W_{lm}} \left( W u_k \right) \nonumber \\
	&= X_i \frac{\partial}{\partial W_{lm}} \left( u^T_k W^T \right) \nonumber \\
	&= X_i u^T_k \frac{\partial}{\partial W_{lm}} \left(  W^T \right) \label{eq:13}
\end{align}
\begin{align}
	\frac{\partial}{\partial W_{lm}} \left(  W^T \right) &= \omega^{T}_{lm} \label{eq:14}
\end{align}
Where $\omega_{lm}$ is a matrix full of zeros except at $index_{lm}$ which is 1
By substitution from equation \eqref{eq:14} in equation \eqref{eq:13} we get:
\begin{align}
{\left( \nabla_W S_{ik} \right)}_{lm} &= X_i u^T_k \omega^T_{lm} \nonumber \\  
& = X_i \omega_{lm} u_k \label{eq:15}
\end{align}
\begin{align}
\omega_{lm} u_k &= \begin{cases}
0 & k \ne l \\
u_m & k = l
\end{cases} \label{eq:16}
\end{align}
So by getting the right hand side from equation \eqref{eq:15} and substituting by equation \eqref{eq:16} we get: 
\begin{align}
X_i \omega_{lm} u_k &= \begin{cases}
0 & k \ne l \\
X_i u_m = X_{im} & k = l
\end{cases} \label{eq:17}
\end{align} 
Where $X_{im}$ is m\textsuperscript{th} element in $X_i$ \newline
So from equation \eqref{eq:15} and equation \eqref{eq:17} we get:
\begin{align}
	\nabla_W \left( S_{ik} \right) &= \left[\begin{array}{cccc}
	0 & 0 & | & 0 \\
	0 & 0 & X_i & 0 \\
	0 & 0 & | & 0 \\
	0 & 0 & k_{th}-col & 0 \\
	\end{array}\right] \label{eq:18}
\end{align}
\begin{align}
	\text{Simlarly } \nabla_W \left( S_{y_j} \right) &= \left[\begin{array}{cccc}
	0 & 0 & | & 0 \\
	0 & 0 & X_i & 0 \\
	0 & 0 & | & 0 \\
	0 & 0 & y_{j-th} col & 0 \\
	\end{array}\right] \label{eq:19}
\end{align}
By substituting from equation \eqref{eq:18} and \eqref{eq:19} in equation \eqref{eq:3} we get:
\begin{align}
	\text{So } \nabla_W T_{ik} &= \begin{cases}
	\left[\begin{array}{cccc}
	| & 0 & | & 0 \\
	X_i & 0 & X_i & 0 \\
	| & 0 & | & 0 \\
	k_{th}-col & 0 & y_{j-th}-col & 0 \\
	\end{array}\right] & \text{, } margin_{ik} \le 0 \\
	\text{zero-matrix} & \text{, } margin_{ik} > 0 \\
	\end{cases} \label{eq:20}
\end{align}
From equation \eqref{eq:20} in \eqref{eq:2} we get:
\begin{align}
	\nabla_W loss_{i} &= \left[\begin{array}{cccc}
	| & 0 & | & 0 \\
	\text{$X_i$ if $morgin_{ik} > 0$  else zeros} & 0 & -m_i X_i & 0 \\
	| & 0 & | & 0 \\
	k_{th}-col & 0 & y_{j-th}-col & 0 \\
	\end{array}\right]
\end{align}
Where $m_i$ is the number of $margin_{ik}$ in i\textsuperscript{th} example that is greater than zero, \newline So we can assume C the cost matrix where $C_{in}$ corresponds to the i\textsuperscript{th} example and c\textsuperscript{th} class cost where
\begin{align}
	C_{in} &= \begin{cases}
	-m_i & n=y_j \text{ n is the correct label for $example_i$}\\
	1 & margin_{in} > 0 \\
	0 & \text{otherwise} \\
	\end{cases}
\end{align}
And we get:
\begin{align}
	\left(\sum_{i}^{N} \nabla_W loss_i\right)_{lm} &= \sum_{i}^{N} C_{im} X_{il} 
\end{align}
\begin{align}
	\text{So } \sum_{i}^{N} \nabla_W loss_i &= X^T C \label{eq:24}
\end{align}
And from \eqref{eq:24} and \eqref{eq:9} and substituting in \eqref{eq:8} we get the final vectorized loss expression:
\begin{align}
	 \nabla_W loss &= \frac{1}{N} \left( X^T C \right) +  2 \lambda W
\end{align} 


\end{document}
